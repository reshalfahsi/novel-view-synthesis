{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "1vsZ0CSUqQLP",
        "fMVZ1KzTDA48",
        "dGXboU5pDD7_",
        "vpiovq5V_Lyb",
        "QBd8FH5rpbNF",
        "qI32fAKB-sX7",
        "tTPvUJpRpbYW",
        "dLBrWDMqpbjQ",
        "M5um-g8RF-Du",
        "sY9ev-TpwUTM",
        "xL6_8zKppbuL",
        "wRbfMQeipb4j",
        "RYw3fv3epcCS",
        "IRRZa2gRpcNK",
        "AQIIjEv3q2o8",
        "xNdnytwzZVuy",
        "IkT5xSnAZW5e"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reshalfahsi/novel-view-synthesis/blob/master/Novel_View_Synthesis_Using_NeRF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Novel View Synthesis Using NeRF**"
      ],
      "metadata": {
        "id": "SzBEGG2epaJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Important Libraries**"
      ],
      "metadata": {
        "id": "1vsZ0CSUqQLP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Install**"
      ],
      "metadata": {
        "id": "fMVZ1KzTDA48"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOUdEvuEpVdc"
      },
      "outputs": [],
      "source": [
        "!pip install -q --no-cache-dir lightning torchmetrics moviepy wget"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Import**"
      ],
      "metadata": {
        "id": "dGXboU5pDD7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import lightning as L\n",
        "except:\n",
        "    import lightning as L\n",
        "\n",
        "from lightning.pytorch import Trainer, seed_everything\n",
        "from lightning.pytorch.callbacks import ModelCheckpoint\n",
        "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
        "\n",
        "from torchmetrics.image import (\n",
        "    PeakSignalNoiseRatio,\n",
        "    StructuralSimilarityIndexMeasure\n",
        ")\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "from IPython.display import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import imageio.v2 as imageio\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "from glob import glob\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import wget\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# %matplotlib inline\n",
        "plt.rcParams['axes.facecolor'] = 'lightgray'\n",
        "plt.rcParams['mathtext.fontset'] = 'cm'\n",
        "plt.rcParams['font.family'] = 'STIXGeneral'"
      ],
      "metadata": {
        "id": "jrj3mPTFqZfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Configuration**"
      ],
      "metadata": {
        "id": "vpiovq5V_Lyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LEARNING_RATE = 3.1e-4\n",
        "NEAR = 2.\n",
        "BATCH_SIZE = 4\n",
        "FAR = 6.\n",
        "POS_ENCODE_DIMS = 16\n",
        "NUM_SAMPLES = 32\n",
        "NeRF_WIDTH = 108\n",
        "EPOCH = 1600"
      ],
      "metadata": {
        "id": "Tn5D-D5k_L7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dataset**"
      ],
      "metadata": {
        "id": "QBd8FH5rpbNF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Download and Process**"
      ],
      "metadata": {
        "id": "qI32fAKB-sX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists('tiny_nerf_data.npz'):\n",
        "    DATASET_URL = (\n",
        "        \"http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/tiny_nerf_data.npz\"\n",
        "    )\n",
        "    wget.download(DATASET_URL, 'tiny_nerf_data.npz')\n",
        "\n",
        "dataset = np.load(\"./tiny_nerf_data.npz\")\n",
        "\n",
        "IMAGES = dataset[\"images\"]\n",
        "POSES = dataset[\"poses\"]\n",
        "FOCAL = float(dataset[\"focal\"])\n",
        "\n",
        "del dataset"
      ],
      "metadata": {
        "id": "t3WeV0F9pbSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Utils**"
      ],
      "metadata": {
        "id": "tTPvUJpRpbYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ViewSynthesisDataset(data.Dataset):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.split = None\n",
        "\n",
        "        self.num_images, self.H, self.W, C = IMAGES.shape\n",
        "        self._split_index = int(0.8 * self.num_images)\n",
        "\n",
        "    def __call__(self, split=None):\n",
        "        assert (\n",
        "            split is not None\n",
        "        ), \"Please specify dataset split: train, val, and test\"\n",
        "        self.split = split\n",
        "        return self\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.split is None:\n",
        "            raise KeyError(\"'split' is not defined yet\")\n",
        "        if self.split == \"train\":\n",
        "            return len(IMAGES[: int(0.8 * self._split_index)])\n",
        "        elif self.split == \"val\":\n",
        "            return len(IMAGES[int(0.8 * self._split_index) : self._split_index])\n",
        "        elif self.split == \"test\":\n",
        "            return len(IMAGES[self._split_index : self.num_images])\n",
        "\n",
        "    def get_rays(self, pose):\n",
        "        xs = torch.arange(self.W) - (self.W / 2 - 0.5)\n",
        "        ys = torch.arange(self.H) - (self.H / 2 - 0.5)\n",
        "        xs, ys = torch.meshgrid(xs, -ys, indexing=\"xy\")\n",
        "\n",
        "        zs = torch.full_like(xs, -FOCAL)\n",
        "\n",
        "        directions = torch.stack(\n",
        "            [xs, ys, zs], dim=-1\n",
        "        )\n",
        "        directions = directions / FOCAL\n",
        "\n",
        "        # Get the camera matrix.\n",
        "        camera_matrix = pose[:3, :3]\n",
        "        height_width_focal = pose[:3, -1]\n",
        "\n",
        "        ray_directions = torch.einsum(\"ij,hwj->hwi\", camera_matrix, directions)\n",
        "        ray_origins = height_width_focal.expand(ray_directions.shape)\n",
        "\n",
        "        return ray_origins, ray_directions\n",
        "\n",
        "    def render_rays(self, ray_origins, ray_directions):\n",
        "        t_vals = torch.linspace(NEAR, FAR, NUM_SAMPLES)\n",
        "\n",
        "        noise = torch.rand(\n",
        "            torch.Size(list(ray_origins.shape[:-1]) + [NUM_SAMPLES])\n",
        "        ) * (FAR - NEAR) / NUM_SAMPLES\n",
        "        t_vals = t_vals + noise\n",
        "\n",
        "        # Equation: r(t) = o + td -> Building the \"r\" here.\n",
        "        rays = ray_origins[..., None, :] + (\n",
        "            ray_directions[..., None, :] * t_vals[..., None]\n",
        "        )\n",
        "\n",
        "        return rays, t_vals\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.split is None:\n",
        "            raise KeyError(\"'split' is not defined yet\")\n",
        "        if self.split == \"train\":\n",
        "            image = torch.Tensor(\n",
        "                IMAGES[: int(0.8 * self._split_index)][idx]\n",
        "            )\n",
        "            pose = torch.Tensor(\n",
        "                POSES[: int(0.8 * self._split_index)][idx]\n",
        "            )\n",
        "        elif self.split == \"val\":\n",
        "            image = torch.Tensor(\n",
        "                IMAGES[int(0.8 * self._split_index) : self._split_index][idx]\n",
        "            )\n",
        "            pose = torch.Tensor(\n",
        "                POSES[int(0.8 * self._split_index) : self._split_index][idx]\n",
        "            )\n",
        "        elif self.split == \"test\":\n",
        "            image = torch.Tensor(\n",
        "                IMAGES[self._split_index : self.num_images][idx]\n",
        "            )\n",
        "            pose = torch.Tensor(\n",
        "                POSES[self._split_index : self.num_images][idx]\n",
        "            )\n",
        "\n",
        "        ray_origins, ray_directions = self.get_rays(pose)\n",
        "        rays, t_vals = self.render_rays(ray_origins, ray_directions)\n",
        "\n",
        "        return image, pose, rays, t_vals"
      ],
      "metadata": {
        "id": "ttV37fywpbdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ViewSynthesisDataset = ViewSynthesisDataset()"
      ],
      "metadata": {
        "id": "ysF--y3532Ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model**"
      ],
      "metadata": {
        "id": "dLBrWDMqpbjQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Utils**"
      ],
      "metadata": {
        "id": "M5um-g8RF-Du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AvgMeter(object):\n",
        "    def __init__(self, complete=False):\n",
        "        self.reset()\n",
        "        self.complete = complete\n",
        "\n",
        "    def reset(self):\n",
        "        self.scores = []\n",
        "\n",
        "    def update(self, val):\n",
        "        self.scores.append(val)\n",
        "\n",
        "    @property\n",
        "    def score(self):\n",
        "        score = [s.numpy() for s in self.scores]\n",
        "        return score\n",
        "\n",
        "    def show(self):\n",
        "        scores = torch.stack(self.scores)\n",
        "\n",
        "        if self.complete:\n",
        "            mean = torch.mean(scores)\n",
        "            std = torch.std(scores)\n",
        "            return mean, std\n",
        "        else:\n",
        "            return torch.mean(scores)"
      ],
      "metadata": {
        "id": "cspQxhwWpbo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Positional Encoding**"
      ],
      "metadata": {
        "id": "sY9ev-TpwUTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, length):\n",
        "        super().__init__()\n",
        "        self.length = length\n",
        "\n",
        "    def forward(self, x):\n",
        "        _x = [x]\n",
        "        for l in range(self.length):\n",
        "            _x.append(torch.sin(2 ** l * torch.pi * x))\n",
        "            _x.append(torch.cos(2 ** l * torch.pi * x))\n",
        "        return torch.cat(_x, dim=-1)"
      ],
      "metadata": {
        "id": "Tip-qx-mwUeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **NeRF**"
      ],
      "metadata": {
        "id": "xL6_8zKppbuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeRF(nn.Module):\n",
        "    def __init__(self, bias=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.positional_encoding = PositionalEncoding(POS_ENCODE_DIMS)\n",
        "\n",
        "        self.mlp0 = nn.Sequential(\n",
        "            nn.Linear(3 + 3 * 2 * POS_ENCODE_DIMS, NeRF_WIDTH, bias=bias),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(NeRF_WIDTH, NeRF_WIDTH, bias=bias),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(NeRF_WIDTH, NeRF_WIDTH, bias=bias),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "\n",
        "        self.mlp1 = nn.Sequential(\n",
        "            nn.Linear(3 + 3 * 2 * POS_ENCODE_DIMS + NeRF_WIDTH, NeRF_WIDTH, bias=bias),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(NeRF_WIDTH, NeRF_WIDTH, bias=bias),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "\n",
        "        self.sigma = nn.Sequential(\n",
        "            nn.Linear(NeRF_WIDTH, NeRF_WIDTH // 2, bias=bias),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(NeRF_WIDTH // 2, 1),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "        )\n",
        "        self.color = nn.Sequential(\n",
        "            nn.Linear(NeRF_WIDTH, NeRF_WIDTH // 2, bias=bias),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(NeRF_WIDTH // 2, 3),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, xs):\n",
        "        xs = self.positional_encoding(xs)\n",
        "\n",
        "        output = self.mlp0(xs)\n",
        "        output = self.mlp1(torch.cat([xs, output], dim=-1))\n",
        "        sigma_output = self.sigma(output)\n",
        "        color_output = self.color(output)\n",
        "\n",
        "        return color_output, sigma_output"
      ],
      "metadata": {
        "id": "sfFE5Z5GpbzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = NeRF\n",
        "MODEL_NAME = MODEL.__name__"
      ],
      "metadata": {
        "id": "yA6nJoAqsNDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Wrapper**"
      ],
      "metadata": {
        "id": "wRbfMQeipb4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelWrapper(L.LightningModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        arch,\n",
        "        lr,\n",
        "        batch_size,\n",
        "        max_epoch,\n",
        "        n_visualization_frames=200,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.arch = arch\n",
        "\n",
        "        self.lr = lr\n",
        "        self.lr_now = 1e1\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.max_epoch = max_epoch\n",
        "\n",
        "        self.automatic_optimization = False\n",
        "\n",
        "        self.train_psnr = PeakSignalNoiseRatio(data_range=1.0)\n",
        "        self.train_ssim = StructuralSimilarityIndexMeasure(data_range=1.0)\n",
        "\n",
        "        self.val_psnr = PeakSignalNoiseRatio(data_range=1.0)\n",
        "        self.val_ssim = StructuralSimilarityIndexMeasure(data_range=1.0)\n",
        "\n",
        "        self.test_psnr = PeakSignalNoiseRatio(data_range=1.0)\n",
        "        self.test_ssim = StructuralSimilarityIndexMeasure(data_range=1.0)\n",
        "\n",
        "        self.train_loss = list()\n",
        "        self.val_loss = list()\n",
        "\n",
        "        self.train_psnr_list = list()\n",
        "        self.train_ssim_list = list()\n",
        "\n",
        "        self.val_psnr_list = list()\n",
        "        self.val_ssim_list = list()\n",
        "\n",
        "        self.train_loss_recorder = AvgMeter()\n",
        "        self.val_loss_recorder = AvgMeter()\n",
        "\n",
        "        self.train_psnr_recorder = AvgMeter()\n",
        "        self.train_ssim_recorder = AvgMeter()\n",
        "\n",
        "        self.val_psnr_recorder = AvgMeter()\n",
        "        self.val_ssim_recorder = AvgMeter()\n",
        "\n",
        "        self.test_psnr_recorder = AvgMeter()\n",
        "        self.test_ssim_recorder = AvgMeter()\n",
        "\n",
        "        self.loss_function = nn.MSELoss()\n",
        "\n",
        "        self.sanity_check_counter = 1\n",
        "        self.n_visualization_frames = n_visualization_frames\n",
        "\n",
        "    def forward(self, ray, t_vals):\n",
        "        color, sigma = self.arch(ray)\n",
        "        color = color.reshape(\n",
        "            ray.shape[0],\n",
        "            ViewSynthesisDataset.H,\n",
        "            ViewSynthesisDataset.W,\n",
        "            NUM_SAMPLES,\n",
        "            3,\n",
        "        )\n",
        "        sigma = sigma.reshape(\n",
        "            ray.shape[0],\n",
        "            ViewSynthesisDataset.H,\n",
        "            ViewSynthesisDataset.W,\n",
        "            NUM_SAMPLES,\n",
        "            1,\n",
        "        )\n",
        "\n",
        "        # Get the distance of adjacent intervals.\n",
        "        delta = t_vals[..., 1:] - t_vals[..., :-1]\n",
        "        delta = torch.concat(\n",
        "            [\n",
        "                delta,\n",
        "                torch.full(\n",
        "                    (\n",
        "                        ray.shape[0],\n",
        "                        ViewSynthesisDataset.H,\n",
        "                        ViewSynthesisDataset.W,\n",
        "                        1,\n",
        "                    ),\n",
        "                    1e10,\n",
        "                ).to(\"cpu\" if not torch.cuda.is_available() else \"cuda\"),\n",
        "            ],\n",
        "            dim=-1,\n",
        "        )\n",
        "\n",
        "        alpha = 1.0 - torch.exp(-sigma * delta[..., None])\n",
        "\n",
        "        # Get transmittance.\n",
        "        exp_term = 1.0 - alpha\n",
        "        epsilon = 1e-10\n",
        "        transmittance = torch.cumprod(exp_term + epsilon, dim=-1)\n",
        "        weights = alpha * transmittance\n",
        "\n",
        "        color = torch.sum(weights * color, dim=-2)\n",
        "        depth = torch.sum(weights * t_vals[..., None], dim=-2)\n",
        "\n",
        "        return color, depth\n",
        "\n",
        "    def training_step(self, batch, batch_nb):\n",
        "        image, pose, rays, t_vals = batch\n",
        "\n",
        "        color, _ = self(rays, t_vals)\n",
        "\n",
        "        loss = self.loss_function(color.permute(0, 3, 1, 2), image.permute(0, 3, 1, 2))\n",
        "\n",
        "        opt = self.optimizers()\n",
        "        opt.zero_grad()\n",
        "        self.manual_backward(loss)\n",
        "        opt.step()\n",
        "\n",
        "        self.log(\"train_loss\", loss, prog_bar=True)\n",
        "        self.train_loss_recorder.update(loss.data)\n",
        "\n",
        "        self.train_psnr.update(color.permute(0, 3, 1, 2), image.permute(0, 3, 1, 2))\n",
        "        psnr = self.train_psnr.compute().data.cpu()\n",
        "\n",
        "        self.train_ssim.update(color.permute(0, 3, 1, 2), image.permute(0, 3, 1, 2))\n",
        "        ssim = self.train_ssim.compute().data.cpu()\n",
        "\n",
        "        self.log(\"train_psnr\", psnr, prog_bar=True)\n",
        "        self.log(\"train_ssim\", ssim, prog_bar=True)\n",
        "\n",
        "        self.train_psnr_recorder.update(psnr)\n",
        "        self.train_ssim_recorder.update(ssim)\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        self.train_loss.append(self.train_loss_recorder.show().data.cpu().numpy())\n",
        "        self.train_loss_recorder = AvgMeter()\n",
        "        self.train_psnr_list.append(self.train_psnr_recorder.show().data.cpu().numpy())\n",
        "        self.train_psnr_recorder = AvgMeter()\n",
        "        self.train_ssim_list.append(self.train_ssim_recorder.show().data.cpu().numpy())\n",
        "        self.train_ssim_recorder = AvgMeter()\n",
        "\n",
        "    def validation_step(self, batch, batch_nb):\n",
        "        image, pose, rays, t_vals = batch\n",
        "\n",
        "        color, depth = self(rays, t_vals)\n",
        "\n",
        "        loss = self.loss_function(color.permute(0, 3, 1, 2), image.permute(0, 3, 1, 2))\n",
        "\n",
        "        # Plot the rgb, depth and the loss plot.\n",
        "        if self.sanity_check_counter == 0:\n",
        "            self.log(\"val_loss\", loss, prog_bar=True)\n",
        "            self.val_loss_recorder.update(loss.data)\n",
        "\n",
        "            self.val_psnr.update(color.permute(0, 3, 1, 2), image.permute(0, 3, 1, 2))\n",
        "            psnr = self.val_psnr.compute().data.cpu()\n",
        "\n",
        "            self.val_ssim.update(color.permute(0, 3, 1, 2), image.permute(0, 3, 1, 2))\n",
        "            ssim = self.val_ssim.compute().data.cpu()\n",
        "\n",
        "            self.log(\"val_psnr\", psnr, prog_bar=True)\n",
        "            self.log(\"val_ssim\", ssim, prog_bar=True)\n",
        "\n",
        "            self.val_psnr_recorder.update(psnr)\n",
        "            self.val_ssim_recorder.update(ssim)\n",
        "\n",
        "            if (self.current_epoch + 1) % (\n",
        "                self.max_epoch // self.n_visualization_frames\n",
        "            ) == 0:\n",
        "                fig, ax = plt.subplots(\n",
        "                    nrows=1,\n",
        "                    ncols=4,\n",
        "                    figsize=(22, 4.8),\n",
        "                )\n",
        "\n",
        "                image = image[0].data.cpu().numpy()\n",
        "                ax[0].imshow(np.clip(image, 0.0, 1.0))\n",
        "                ax[0].set_title(f\"Ground Truth: {self.current_epoch}\")\n",
        "\n",
        "                color = color[0].data.cpu().numpy()\n",
        "                ax[1].imshow(np.clip(color, 0.0, 1.0))\n",
        "                ax[1].set_title(f\"Predicted: {self.current_epoch}\")\n",
        "\n",
        "                depth = depth[0].data.cpu().numpy()\n",
        "                ax[2].imshow(np.clip(depth, 0.0, 1.0))\n",
        "                ax[2].set_title(f\"Depth Map: {self.current_epoch}\")\n",
        "\n",
        "                ax[3].plot(self.val_loss)\n",
        "                ax[3].grid()\n",
        "                ax[3].set_title(f\"Loss Plot: {self.current_epoch}\")\n",
        "\n",
        "                fig.tight_layout()\n",
        "                fig.savefig(\n",
        "                    f\"experiment/training/images/{str(self.current_epoch).zfill(5)}.png\"\n",
        "                )\n",
        "                fig.clf()\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        if self.sanity_check_counter == 0:\n",
        "            psnr = self.val_psnr_recorder.show()\n",
        "            sch = self.lr_schedulers()\n",
        "            sch.step(psnr)\n",
        "\n",
        "            lr_now_ = self.optimizers().param_groups[0][\"lr\"]\n",
        "            if self.lr_now > lr_now_:\n",
        "                self.lr_now = lr_now_\n",
        "                print(\n",
        "                    f\"[{MODEL_NAME}] Learning Rate Changed: {lr_now_} - Epoch: {self.current_epoch}\"\n",
        "                )\n",
        "\n",
        "            self.val_loss.append(self.val_loss_recorder.show().data.cpu().numpy())\n",
        "            self.val_loss_recorder = AvgMeter()\n",
        "            self.val_psnr_list.append(psnr.data.cpu().numpy())\n",
        "            self.val_psnr_recorder = AvgMeter()\n",
        "            self.val_ssim_list.append(self.val_ssim_recorder.show().data.cpu().numpy())\n",
        "            self.val_ssim_recorder = AvgMeter()\n",
        "        else:\n",
        "            self.sanity_check_counter -= 1\n",
        "\n",
        "    def test_step(self, batch, batch_nb):\n",
        "        image, pose, rays, t_vals = batch\n",
        "\n",
        "        color, _ = self(rays, t_vals)\n",
        "\n",
        "        self.test_psnr.update(color.permute(0, 3, 1, 2), image.permute(0, 3, 1, 2))\n",
        "        psnr = self.test_psnr.compute().data.cpu()\n",
        "\n",
        "        self.test_ssim.update(color.permute(0, 3, 1, 2), image.permute(0, 3, 1, 2))\n",
        "        ssim = self.test_ssim.compute().data.cpu()\n",
        "\n",
        "        self.log(\"test_psnr\", psnr, prog_bar=True)\n",
        "        self.log(\"test_ssim\", ssim, prog_bar=True)\n",
        "\n",
        "        self.test_psnr_recorder.update(psnr)\n",
        "        self.test_ssim_recorder.update(ssim)\n",
        "\n",
        "    def on_train_end(self):\n",
        "        # Loss\n",
        "        loss_img_file = f\"experiment/training/{MODEL_NAME}_loss_plot.png\"\n",
        "        plt.figure(figsize=(6.4, 4.8))\n",
        "        plt.plot(self.train_loss, color=\"r\", label=\"train\")\n",
        "        plt.plot(self.val_loss, color=\"b\", label=\"validation\")\n",
        "        plt.title(\"Loss Curves\")\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.legend()\n",
        "        plt.grid()\n",
        "        plt.savefig(loss_img_file)\n",
        "        plt.clf()\n",
        "\n",
        "        # Evaluation Metrics\n",
        "        evaluation_metric_img_file = f\"experiment/training/{MODEL_NAME}_psnr_plot.png\"\n",
        "        plt.figure(figsize=(6.4, 4.8))\n",
        "        plt.plot(self.train_psnr_list, color=\"r\", label=\"train\")\n",
        "        plt.plot(self.val_psnr_list, color=\"b\", label=\"validation\")\n",
        "        plt.title(\"PSNR Curves\")\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"PSNR\")\n",
        "        plt.legend()\n",
        "        plt.grid()\n",
        "        plt.savefig(evaluation_metric_img_file)\n",
        "        plt.clf()\n",
        "\n",
        "        evaluation_metric_img_file = f\"experiment/training/{MODEL_NAME}_ssim_plot.png\"\n",
        "        plt.figure(figsize=(6.4, 4.8))\n",
        "        plt.plot(self.train_ssim_list, color=\"r\", label=\"train\")\n",
        "        plt.plot(self.val_ssim_list, color=\"b\", label=\"validation\")\n",
        "        plt.title(\"SSIM Curves\")\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"SSIM\")\n",
        "        plt.legend()\n",
        "        plt.grid()\n",
        "        plt.savefig(evaluation_metric_img_file)\n",
        "        plt.clf()\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return data.DataLoader(\n",
        "            dataset=ViewSynthesisDataset(\"train\"),\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True,\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return data.DataLoader(\n",
        "            dataset=ViewSynthesisDataset(\"val\"),\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False,\n",
        "        )\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.Adam(\n",
        "            self.arch.parameters(),\n",
        "            lr=self.lr,\n",
        "        )\n",
        "\n",
        "        lr_scheduler = {\n",
        "            \"scheduler\": optim.lr_scheduler.ReduceLROnPlateau(\n",
        "                optimizer,\n",
        "                mode=\"max\",\n",
        "                factor=0.25,\n",
        "                patience=int(0.025 * self.max_epoch),\n",
        "            ),\n",
        "            \"name\": \"lr_scheduler\",\n",
        "        }\n",
        "\n",
        "        return [optimizer], [lr_scheduler]"
      ],
      "metadata": {
        "id": "xs-hrXUdpb9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WRAPPER = ModelWrapper"
      ],
      "metadata": {
        "id": "KF9lMGCwMqx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EXPERIMENT_DIR = \"experiment/\"\n",
        "BEST_MODEL_PATH = os.path.join(EXPERIMENT_DIR, f\"{MODEL_NAME}_best.ckpt\")"
      ],
      "metadata": {
        "id": "81wYlpIHMwBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training**"
      ],
      "metadata": {
        "id": "RYw3fv3epcCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"experiment\", exist_ok=True)\n",
        "os.makedirs(\"experiment/training\", exist_ok=True)\n",
        "os.makedirs(\"experiment/training/images\", exist_ok=True)"
      ],
      "metadata": {
        "id": "5C8JRNs6pcHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = int(np.random.randint(2147483647))\n",
        "print(f\"Random seed: {SEED}\")"
      ],
      "metadata": {
        "id": "ZMqw7ETNLgUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, trainer, checkpoint, early_stopping = None, None, None, None\n",
        "\n",
        "\n",
        "def _train_loop():\n",
        "    seed_everything(SEED, workers=True)\n",
        "\n",
        "    model = MODEL()\n",
        "    model = WRAPPER(model, LEARNING_RATE, BATCH_SIZE, EPOCH)\n",
        "\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        monitor='val_psnr',\n",
        "        dirpath=EXPERIMENT_DIR,\n",
        "        mode='max',\n",
        "        filename=f\"{MODEL_NAME}_best\",\n",
        "    )\n",
        "    print(MODEL_NAME)\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor=\"val_psnr\",\n",
        "        min_delta=0.00,\n",
        "        patience=int(0.1 * EPOCH),\n",
        "        verbose=False,\n",
        "        mode=\"max\",\n",
        "    )\n",
        "\n",
        "    if os.path.exists(BEST_MODEL_PATH):\n",
        "        ckpt_path = BEST_MODEL_PATH\n",
        "    else:\n",
        "        ckpt_path = None\n",
        "\n",
        "    trainer = Trainer(\n",
        "        accelerator=\"auto\",\n",
        "        devices=1,\n",
        "        max_epochs=EPOCH,\n",
        "        logger=False,\n",
        "        callbacks=[checkpoint, early_stopping],\n",
        "        log_every_n_steps=5,\n",
        "    )\n",
        "    trainer.fit(model, ckpt_path=ckpt_path)\n",
        "\n",
        "_train_loop()"
      ],
      "metadata": {
        "id": "A0mPvh1LLgbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = cv2.imread(f\"experiment/training/{MODEL_NAME}_loss_plot.png\")\n",
        "cv2_imshow(img)\n",
        "\n",
        "img = cv2.imread(f\"experiment/training/{MODEL_NAME}_psnr_plot.png\")\n",
        "cv2_imshow(img)\n",
        "\n",
        "img = cv2.imread(f\"experiment/training/{MODEL_NAME}_ssim_plot.png\")\n",
        "cv2_imshow(img)\n",
        "\n",
        "filenames = glob(\"experiment/training/images/*.png\")\n",
        "filenames = sorted(filenames)\n",
        "image = list()\n",
        "for filename in tqdm(filenames):\n",
        "    image.append(imageio.imread(filename))\n",
        "kargs = {\n",
        "    \"duration\": 1.25,\n",
        "    \"loop\": 0,\n",
        "}\n",
        "imageio.mimsave(\"experiment/training/result.gif\", image, \"GIF\", **kargs)\n",
        "with open(\"experiment/training/result.gif\", \"rb\") as f:\n",
        "    display(Image(data=f.read(), format=\"gif\"))"
      ],
      "metadata": {
        "id": "VWK5_RIBH0Bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Testing**"
      ],
      "metadata": {
        "id": "IRRZa2gRpcNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(accelerator='auto', logger=False)\n",
        "model = MODEL()\n",
        "model = WRAPPER(model, LEARNING_RATE, BATCH_SIZE, EPOCH)\n",
        "trainer.test(\n",
        "    model=model,\n",
        "    ckpt_path=BEST_MODEL_PATH,\n",
        "    dataloaders=data.DataLoader(\n",
        "        dataset=ViewSynthesisDataset(\"test\"),\n",
        "        batch_size=1,\n",
        "        shuffle=False,\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "PGDLwlPsP_kM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Inference**"
      ],
      "metadata": {
        "id": "AQIIjEv3q2o8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Utils**"
      ],
      "metadata": {
        "id": "xNdnytwzZVuy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_translation_t(t):\n",
        "    \"\"\"Get the translation matrix for movement in t.\"\"\"\n",
        "    matrix = np.array([\n",
        "        [1, 0, 0, 0],\n",
        "        [0, 1, 0, 0],\n",
        "        [0, 0, 1, t],\n",
        "        [0, 0, 0, 1],\n",
        "    ])\n",
        "    return torch.tensor(matrix, dtype=torch.float32)\n",
        "\n",
        "\n",
        "def get_rotation_phi(phi):\n",
        "    \"\"\"Get the rotation matrix for movement in phi.\"\"\"\n",
        "    matrix = np.array([\n",
        "        [1, 0, 0, 0],\n",
        "        [0, math.cos(phi), -math.sin(phi), 0],\n",
        "        [0, math.sin(phi), math.cos(phi), 0],\n",
        "        [0, 0, 0, 1],\n",
        "    ])\n",
        "    return torch.tensor(matrix, dtype=torch.float32)\n",
        "\n",
        "\n",
        "def get_rotation_theta(theta):\n",
        "    \"\"\"Get the rotation matrix for movement in theta.\"\"\"\n",
        "    matrix = np.array([\n",
        "        [math.cos(theta), 0, -math.sin(theta), 0],\n",
        "        [0, 1, 0, 0],\n",
        "        [math.sin(theta), 0, math.cos(theta), 0],\n",
        "        [0, 0, 0, 1],\n",
        "    ])\n",
        "    return torch.tensor(matrix, dtype=torch.float32)\n",
        "\n",
        "\n",
        "def pose_spherical(theta, phi, t):\n",
        "    \"\"\"\n",
        "    Get the camera to world matrix for the corresponding theta, phi\n",
        "    and t.\n",
        "    \"\"\"\n",
        "    c2w = get_translation_t(t)\n",
        "    c2w = get_rotation_phi(phi / 180.0 * np.pi) @ c2w\n",
        "    c2w = get_rotation_theta(theta / 180.0 * np.pi) @ c2w\n",
        "    c2w = torch.tensor(\n",
        "        np.array(\n",
        "            [\n",
        "                [-1, 0, 0, 0],\n",
        "                [0, 0, 1, 0],\n",
        "                [0, 1, 0, 0],\n",
        "                [0, 0, 0, 1],\n",
        "            ]\n",
        "        ),\n",
        "        dtype=torch.float32\n",
        "    ) @ c2w\n",
        "    return c2w"
      ],
      "metadata": {
        "id": "CFvnWVVyq2tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Render**"
      ],
      "metadata": {
        "id": "IkT5xSnAZW5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frames = list()\n",
        "batch_ray = list()\n",
        "batch_t = list()\n",
        "\n",
        "\n",
        "PHI = -15.0\n",
        "CAMERA_DIST = 3.5\n",
        "DELTA_THETA = 3.0\n",
        "\n",
        "\n",
        "model = MODEL()\n",
        "model = WRAPPER.load_from_checkpoint(\n",
        "    BEST_MODEL_PATH,\n",
        "    arch=model,\n",
        "    lr=LEARNING_RATE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    max_epoch=EPOCH,\n",
        ")\n",
        "\n",
        "\n",
        "# Iterate over different theta value and generate scenes.\n",
        "for index, theta in tqdm(\n",
        "    enumerate(np.linspace(0.0, 360.0, int(360./DELTA_THETA), endpoint=False))\n",
        "    ):\n",
        "    # Get the camera to world matrix.\n",
        "    c2w = pose_spherical(theta, PHI, CAMERA_DIST)\n",
        "\n",
        "    ray_orig, ray_dirs = ViewSynthesisDataset.get_rays(c2w)\n",
        "    rays, t_vals = ViewSynthesisDataset.render_rays(ray_orig, ray_dirs)\n",
        "\n",
        "    if index % BATCH_SIZE == 0 and index > 0:\n",
        "        batched_ray = torch.stack(batch_ray, dim=0).to(\n",
        "            \"cpu\" if not torch.cuda.is_available() else \"cuda\"\n",
        "        )\n",
        "        batch_ray = [rays]\n",
        "\n",
        "        batched_t = torch.stack(batch_t, dim=0).to(\n",
        "            \"cpu\" if not torch.cuda.is_available() else \"cuda\"\n",
        "        )\n",
        "        batch_t = [t_vals]\n",
        "\n",
        "        color, _ = model(batched_ray, batched_t)\n",
        "\n",
        "        color = color.data.cpu().numpy()\n",
        "        temp = [\n",
        "            cv2.resize(\n",
        "                np.clip(255 * img, 0.0, 255.0).astype(np.uint8),\n",
        "                (300, 300),\n",
        "            ) for img in color\n",
        "        ]\n",
        "\n",
        "        frames = frames + temp\n",
        "    else:\n",
        "        batch_ray.append(rays)\n",
        "        batch_t.append(t_vals)\n",
        "\n",
        "\n",
        "video_path = \"experiment/result.mp4\"\n",
        "imageio.mimwrite(video_path, frames, fps=30, quality=7, macro_block_size=None)\n",
        "videoClip = VideoFileClip(video_path)\n",
        "videoClip.write_gif(\"experiment/result.gif\")\n",
        "with open(\"experiment/result.gif\", \"rb\") as f:\n",
        "    display(Image(data=f.read(), format=\"gif\"))"
      ],
      "metadata": {
        "id": "DmvWHxybRw96"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
